{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "752860b2",
   "metadata": {},
   "source": [
    "# 1_scrape_core_preview ‚Äî (aggiornato da `3_run_all.ipynb`)\n",
    "\n",
    "Questo notebook esegue **Parte 1**:\n",
    "- Scraping di decklist e matchups\n",
    "- Consolidamento/alias\n",
    "- Costruzione matrici W/L/T/WR e `n_dir`\n",
    "- Filtro NaN e **score_latest/filtered_wr/n_dir**\n",
    "\n",
    "> Fonte di verit√†: il notebook 3. Le celle seguenti sono **estratte e allineate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c5c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# 1) azzera gli handler duplicati di Jupyter ed imposta un‚Äôunica configurazione\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,           # livello generale del progetto\n",
    "    format=\"%(levelname)s:%(name)s:%(message)s\",\n",
    "    force=True                    # üëà IMPORTANTISSIMO in Jupyter per evitare duplicazioni\n",
    ")\n",
    "\n",
    "# 2) silenzia SOLO il logger di rete\n",
    "logging.getLogger(\"ptcgp.net\").setLevel(logging.WARNING)\n",
    "\n",
    "# 3) opzionale: riduci rumore di webdriver-manager / selenium\n",
    "logging.getLogger(\"WDM\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"selenium\").setLevel(logging.WARNING)\n",
    "\n",
    "# 4) se vuoi vedere pi√π diagnostica del tuo codice ma non il traffico rete:\n",
    "logging.getLogger(\"ptcgp\").setLevel(logging.DEBUG)   # tuo codice\n",
    "logging.getLogger(\"utils.io\").setLevel(logging.INFO) # lascia i \"CSV aggiornato\" se ti servono\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa12a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup base e import dei moduli\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from utils.io import init_paths, _dest, write_csv_versioned, save_plot_timestamped\n",
    "from scraper.decklist import scrape_decklist_html, parse_decklist_table, filter_top_meta, LIMITLESS_DECKS_URL\n",
    "from scraper.matchups import to_matchup_url, scrape_matchups\n",
    "from scraper.session import make_session\n",
    "from core.normalize import load_alias_map, build_alias_index\n",
    "from core.consolidate import maxN_flat, apply_alias_and_aggregate, build_score_table_filtered\n",
    "from core.matrices import topmeta_post_alias, build_matrices, n_dir_from_WL\n",
    "from core.nan_filter import filter_wr_nan_iterative\n",
    "\n",
    "# ‚ö†Ô∏è NON richiamare basicConfig: gi√† impostato in cella 1 con force=True\n",
    "log = logging.getLogger(\"ptcgp\")\n",
    "\n",
    "BASE = Path.cwd()\n",
    "paths = init_paths(BASE)\n",
    "paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5191ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garantisci file di configurazione minimi (alias_map.json e config.yaml) e carica CFG\n",
    "cfg_dir = BASE / \"config\"\n",
    "cfg_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "alias_json = cfg_dir / \"alias_map.json\"\n",
    "if not alias_json.exists():\n",
    "    alias_json.write_text(\"{}\\n\", encoding=\"utf-8\")\n",
    "    log.warning(\"[init] creato config/alias_map.json vuoto ‚Äî nessun alias verr√† applicato finch√© non lo compili.\")\n",
    "\n",
    "yaml_file = cfg_dir / \"config.yaml\"\n",
    "if not yaml_file.exists():\n",
    "    yaml_file.write_text(\"# config placeholder\\n\", encoding=\"utf-8\")\n",
    "    log.info(\"[init] creato config/config.yaml placeholder\")\n",
    "\n",
    "# Lettura della config (come facevi prima)\n",
    "import yaml  # se non fosse installato, aggiungi 'pyyaml' al venv/requirements\n",
    "with open(yaml_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = yaml.safe_load(f) or {}\n",
    "log.info(\"Config loaded from %s\", yaml_file)\n",
    "\n",
    "# (facoltativo) Allinea livelli di log alla config\n",
    "lvl = (CFG.get(\"logging\", {}).get(\"level\", \"INFO\") or \"INFO\").upper()\n",
    "logging.getLogger(\"ptcgp\").setLevel(getattr(logging, lvl, logging.INFO))\n",
    "logging.getLogger(\"utils.io\").setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae96156",
   "metadata": {},
   "source": [
    "## D1 ‚Äî Scraping decklist e matchups\n",
    "\n",
    "- Usa Selenium per la pagina **Decks** (cache 12h lato HTML).\n",
    "- Converte in tabella e filtra il **Top-meta (80%)**.\n",
    "- Costruisce le URL `/matchups` e scarica tutte le pagine dei deck top.\n",
    "- Scrive:\n",
    "  - `outputs/Decklists/raw/decklist_raw_*latest.csv`\n",
    "  - `outputs/Decklists/top_meta/top_meta_decklist_*latest.csv`\n",
    "  - `outputs/MatchupData/raw/matchup_raw_*latest.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fda9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esecuzione D1 ‚Äî scraping + salvataggi raw (parametri presi da CFG)\n",
    "\n",
    "# --- Parametri da config.yaml ---\n",
    "scr_cfg   = CFG.get(\"scraping\", {}) if \"CFG\" in globals() else {}\n",
    "top_cfg   = CFG.get(\"top_meta\", {}) if \"CFG\" in globals() else {}\n",
    "\n",
    "DECKS_URL    = scr_cfg.get(\"decks_url\", LIMITLESS_DECKS_URL)\n",
    "TTL_MIN      = int(scr_cfg.get(\"cache_ttl_min\", 720))\n",
    "FORCE_REFRESH= bool(scr_cfg.get(\"force_refresh\", False))\n",
    "HEADLESS     = bool((scr_cfg.get(\"selenium\") or {}).get(\"headless\", True))\n",
    "RATE_LIMIT   = float(scr_cfg.get(\"request_delay_sec\", 5.0))\n",
    "TOP_THRESH   = float(top_cfg.get(\"threshold_pct\", 80.0))\n",
    "\n",
    "log.info(\"[scrape] decks_url=%s | ttl_min=%s | headless=%s | force_refresh=%s | rate_limit=%.2fs | top_thresh=%.1f%%\",\n",
    "         DECKS_URL, TTL_MIN, HEADLESS, FORCE_REFRESH, RATE_LIMIT, TOP_THRESH)\n",
    "\n",
    "# --- Decklist page ---\n",
    "html, from_cache = scrape_decklist_html(\n",
    "    DECKS_URL,\n",
    "    cache_dir=paths.cache, ttl_minutes=TTL_MIN,\n",
    "    force_refresh=FORCE_REFRESH, headless=HEADLESS\n",
    ")\n",
    "df_decklist = parse_decklist_table(html)\n",
    "\n",
    "out1 = write_csv_versioned(\n",
    "    df_decklist.reset_index(),\n",
    "    _dest(paths, \"decklist_raw\"),\n",
    "    \"decklist_raw\",\n",
    "    changed=(not from_cache),\n",
    "    index=False\n",
    ")\n",
    "log.info(\"Decklist rows=%d | saved=%s\", len(df_decklist), out1)\n",
    "\n",
    "# --- Top-meta + URL matchups ---\n",
    "df_top = filter_top_meta(df_decklist, threshold_pct=TOP_THRESH).copy()\n",
    "df_top[\"Matchup URL\"] = df_top[\"URL\"].map(to_matchup_url)\n",
    "\n",
    "out2 = write_csv_versioned(\n",
    "    df_top,\n",
    "    _dest(paths, \"top_meta_decklist\"),\n",
    "    \"top_meta_decklist\",\n",
    "    changed=(not from_cache),\n",
    "    index=False\n",
    ")\n",
    "log.info(\"Top-meta rows=%d | saved=%s\", len(df_top), out2)\n",
    "\n",
    "# --- Scarica matchups per tutti i deck top ---\n",
    "urls = [(r[\"Deck\"], r[\"Matchup URL\"]) for _, r in df_top[[\"Deck\",\"Matchup URL\"]].dropna().iterrows()]\n",
    "if not urls:\n",
    "    raise RuntimeError(\"Nessun URL matchup trovato dal top-meta (controlla la decklist e la colonna 'URL').\")\n",
    "\n",
    "sess = make_session()\n",
    "df_raw, total, cache_hits = scrape_matchups(\n",
    "    urls,\n",
    "    session=sess, cache_dir=paths.cache,\n",
    "    ttl_minutes=TTL_MIN, force_refresh=FORCE_REFRESH,\n",
    "    rate_limit_seconds=RATE_LIMIT,\n",
    "    progress=True,\n",
    "    pbar_desc=f\"Matchups Top {len(urls)}\"\n",
    ")\n",
    "sess.close()\n",
    "\n",
    "# --- Hardening: colonne minime ---\n",
    "_required = {\"Deck A\",\"Deck B\",\"W\",\"L\",\"T\"}\n",
    "missing = _required - set(df_raw.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"[matchup_raw] mancano colonne richieste: {missing}\")\n",
    "\n",
    "out3 = write_csv_versioned(\n",
    "    df_raw,\n",
    "    paths.outputs / \"MatchupData\" / \"raw\",\n",
    "    \"matchup_raw\",\n",
    "    changed=(cache_hits < total) or FORCE_REFRESH,\n",
    "    index=False\n",
    ")\n",
    "log.info(\"Matchup pages=%d | cache hits=%d | rows=%d | saved=%s\", total, cache_hits, len(df_raw), out3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e6e89",
   "metadata": {},
   "source": [
    "## D2/D3 ‚Äî Consolidamento, alias, matrici e filtro NaN\n",
    "\n",
    "Scrive i contratti:\n",
    "- `outputs/MatchupData/flat/score_*latest.csv` *(flat aggregata post-alias con W/L/T/N & WR_dir)*\n",
    "- `outputs/Matrices/winrate/filtered_wr_*latest.csv` *(WR filtrata, diagonale NaN)*\n",
    "- `outputs/Matrices/volumes/n_dir_*latest.csv` *(N_dir = W+L post-aggregazione)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f479ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica l'ultima raw\n",
    "raw_dir = paths.outputs / \"MatchupData\" / \"raw\"\n",
    "raw_latest = max(raw_dir.glob(\"matchup_raw_*latest.csv\"), default=None)\n",
    "if raw_latest is None:\n",
    "    raise SystemExit(\"matchup_raw_latest.csv non trovato: esegui D1\")\n",
    "df_raw = pd.read_csv(raw_latest)\n",
    "\n",
    "# Consolidamento max-N per (A,B)\n",
    "df_flat = maxN_flat(df_raw)\n",
    "\n",
    "# Alias (da config)\n",
    "alias_cfg    = (CFG.get(\"alias\") or {}) if \"CFG\" in globals() else {}\n",
    "APPLY_ALIASES = bool(alias_cfg.get(\"apply\", True))\n",
    "alias_path    = BASE / alias_cfg.get(\"file\", \"config/alias_map.json\")\n",
    "\n",
    "alias_map   = load_alias_map(alias_path) if APPLY_ALIASES else {}\n",
    "alias_index = build_alias_index(alias_map) if APPLY_ALIASES else {}\n",
    "\n",
    "# Apply alias + aggregazione (mirror esclusi) + Winrate direzionale\n",
    "df_flat_alias = apply_alias_and_aggregate(df_flat, alias_index if APPLY_ALIASES else {})\n",
    "\n",
    "# ‚õ≥Ô∏è Salva SOLO la versione pre-filtro come audit (NON deve chiamarsi 'score')\n",
    "flat_dir = _dest(paths, \"matchup_score_table\")\n",
    "write_csv_versioned(df_flat_alias, flat_dir, \"score_pre_filter\", changed=True, index=False)\n",
    "\n",
    "# Asse Top-meta post-alias\n",
    "top_dir    = paths.outputs / \"Decklists\" / \"top_meta\"\n",
    "top_latest = max(top_dir.glob(\"top_meta_decklist_*latest.csv\"), default=None)\n",
    "if top_latest is None:\n",
    "    raise SystemExit(\"top_meta_decklist_latest.csv non trovato: esegui D1\")\n",
    "_df_top     = pd.read_csv(top_latest)\n",
    "df_top_alias= topmeta_post_alias(_df_top, alias_index if APPLY_ALIASES else {})\n",
    "axis        = df_top_alias[\"Deck\"].tolist()\n",
    "\n",
    "# Matrici W/L/T + WR (mode exclude) + n_dir\n",
    "W, L, T, WR = build_matrices(df_flat_alias, axis, mode=\"exclude\", mirror=None)\n",
    "N_DIR = n_dir_from_WL(W, L)\n",
    "\n",
    "# Filtro NaN iterativo (parametri da config)\n",
    "nan_cfg = (CFG.get(\"nan_filter\") or {}) if \"CFG\" in globals() else {}\n",
    "MAX_NAN_RATIO   = float(nan_cfg.get(\"max_nan_ratio\", 0.15))\n",
    "MIN_NAN_ALLOWED = int(nan_cfg.get(\"min_nan_allowed\", 1))\n",
    "USE_CEIL        = bool(nan_cfg.get(\"use_ceil\", False))\n",
    "\n",
    "filtered_wr, dropped = filter_wr_nan_iterative(\n",
    "    WR,\n",
    "    max_nan_ratio=MAX_NAN_RATIO,\n",
    "    min_nan_allowed=MIN_NAN_ALLOWED,\n",
    "    use_ceil=USE_CEIL\n",
    ")\n",
    "kept    = filtered_wr.index.tolist()\n",
    "N_DIR_f = N_DIR.loc[kept, kept]\n",
    "\n",
    "# ‚úÖ Costruisci la score table **post-filtro** sull'asse kept (contratto finale)\n",
    "score_filtered = build_score_table_filtered(\n",
    "    df_flat_alias=df_flat_alias,\n",
    "    kept_axis=kept,\n",
    "    round_wr=2,\n",
    "    legacy_winrate_alias=True  # crea anche 'Winrate' come alias di 'WR_dir'\n",
    ")\n",
    "\n",
    "# Scrivi i contratti (winrate/volumi) + score_latest filtrata\n",
    "win_dir = _dest(paths, \"filtered_wr\")\n",
    "vol_dir = _dest(paths, \"n_dir\")\n",
    "write_csv_versioned(filtered_wr, win_dir, \"filtered_wr\", changed=True, index=True)\n",
    "write_csv_versioned(N_DIR_f,   vol_dir, \"n_dir\",       changed=True, index=True)\n",
    "\n",
    "# üëâ 'score_latest.csv' ORA √® la versione filtrata & simmetrizzata\n",
    "out_score = write_csv_versioned(score_filtered, flat_dir, \"score\", changed=True, index=False)\n",
    "\n",
    "print(\"D2/D3 completati ‚Äî righe flat(pre):\", len(df_flat_alias),\n",
    "      \"| WR shape:\", filtered_wr.shape,\n",
    "      \"| kept:\", len(kept),\n",
    "      \"| score_latest:\", out_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657d053",
   "metadata": {},
   "source": [
    "## Validazione rapida (WR e n_dir)\n",
    "\n",
    "Controlla che:\n",
    "- `filtered_wr ‚âà 100¬∑W/(W+L)` (entro 0.10 pp)\n",
    "- `n_dir == W+L` (off‚Äëdiag) e simmetrico A‚ÜîB\n",
    "- `WR(A,B) + WR(B,A) ‚âà 100` (entro 0.20 pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce9eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ VALIDAZIONE SCORE POST-FILTRO ‚Äî per-riga calcolato con T0 (asse iniziale del filtro)\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor, ceil\n",
    "from utils.io import init_paths\n",
    "\n",
    "EPS_WR  = 0.10  # tolleranza WR (pp)\n",
    "EPS_SUM = 0.20  # tolleranza per WR(A,B)+WR(B,A)‚âà100 (pp)\n",
    "\n",
    "# --- Carica contratti ---\n",
    "paths    = init_paths(Path.cwd())\n",
    "df_score = pd.read_csv(paths.outputs/\"MatchupData\"/\"flat\"/\"score_latest.csv\")\n",
    "df_wr    = pd.read_csv(paths.outputs/\"Matrices\"/\"winrate\"/\"filtered_wr_latest.csv\", index_col=0)\n",
    "df_n     = pd.read_csv(paths.outputs/\"Matrices\"/\"volumes\"/\"n_dir_latest.csv\",   index_col=0)\n",
    "\n",
    "# --- Recupera CFG (gi√† in RAM) o rileggi config.yaml ---\n",
    "try:\n",
    "    CFG\n",
    "except NameError:\n",
    "    import yaml\n",
    "    with open(Path.cwd()/\"config\"/\"config.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "        CFG = yaml.safe_load(f) or {}\n",
    "\n",
    "nan_cfg = (CFG.get(\"nan_filter\") or {})\n",
    "MAX_NAN_RATIO   = float(nan_cfg.get(\"max_nan_ratio\", 0.15))\n",
    "MIN_NAN_ALLOWED = int(nan_cfg.get(\"min_nan_allowed\", 1))\n",
    "USE_CEIL        = bool(nan_cfg.get(\"use_ceil\", False))\n",
    "\n",
    "# --- Asse coerente (finale, post-filtro) ---\n",
    "wr_loaded = df_wr.copy()\n",
    "wr_loaded.index   = wr_loaded.index.astype(str).str.strip()\n",
    "wr_loaded.columns = wr_loaded.columns.astype(str).str.strip()\n",
    "axis = wr_loaded.index.tolist()\n",
    "T = len(axis)  # numero di deck tenuti (kept)\n",
    "\n",
    "# --- Normalizza score & coercizza contatori ---\n",
    "def _coerce_counts(df, cols=(\"W\",\"L\",\"T\",\"N\")):\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0).astype(\"Int64\")\n",
    "    return out\n",
    "\n",
    "req_cols = {\"Deck A\",\"Deck B\",\"W\",\"L\",\"T\",\"N\",\"WR_dir\"}\n",
    "missing = req_cols - set(df_score.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"[score_latest] mancano colonne: {missing}\")\n",
    "\n",
    "score = _coerce_counts(df_score)\n",
    "score[\"Deck A\"] = score[\"Deck A\"].astype(str).str.strip()\n",
    "score[\"Deck B\"] = score[\"Deck B\"].astype(str).str.strip()\n",
    "score = score[score[\"Deck A\"].isin(axis) & score[\"Deck B\"].isin(axis)].copy()\n",
    "\n",
    "# --- Copertura off-diag coerente col filtro ---\n",
    "off_mask        = ~pd.DataFrame(np.eye(T, dtype=bool), index=axis, columns=axis)\n",
    "total_offdiag   = T*(T-1)\n",
    "defined_offdiag = int((wr_loaded.notna() & off_mask).sum().sum())\n",
    "missing_offdiag = total_offdiag - defined_offdiag\n",
    "missing_rate    = (missing_offdiag / total_offdiag) if total_offdiag else 0.0\n",
    "\n",
    "rows = len(score)\n",
    "rows_match_defined = (rows == defined_offdiag)      # lo score copre esattamente le celle definite\n",
    "no_overfill        = (rows <= defined_offdiag)\n",
    "mirrors_ok         = (score[\"Deck A\"] != score[\"Deck B\"]).all()\n",
    "subset_ok          = set(score[\"Deck A\"]).issubset(set(axis)) and set(score[\"Deck B\"]).issubset(set(axis))\n",
    "\n",
    "# --- Calcola allowed_per_row usando T0 (asse iniziale del filtro: top-meta post-alias) ---\n",
    "from core.normalize import load_alias_map, build_alias_index\n",
    "from core.matrices import topmeta_post_alias\n",
    "\n",
    "alias_cfg   = (CFG.get(\"alias\") or {})\n",
    "apply_alias = bool(alias_cfg.get(\"apply\", True))\n",
    "alias_path  = Path.cwd() / alias_cfg.get(\"file\", \"config/alias_map.json\")\n",
    "\n",
    "top_latest = max((paths.outputs/\"Decklists\"/\"top_meta\").glob(\"top_meta_decklist_*latest.csv\"), default=None)\n",
    "if top_latest is None:\n",
    "    raise SystemExit(\"top_meta_decklist_latest.csv non trovato\")\n",
    "\n",
    "_df_top   = pd.read_csv(top_latest)\n",
    "alias_idx = build_alias_index(load_alias_map(alias_path)) if apply_alias else {}\n",
    "axis0     = topmeta_post_alias(_df_top, alias_idx)[\"Deck\"].tolist()\n",
    "T0        = len(axis0)\n",
    "\n",
    "allowed_per_row = max(\n",
    "    MIN_NAN_ALLOWED,\n",
    "    (ceil if USE_CEIL else floor)(MAX_NAN_RATIO * (T0 - 1))\n",
    ")\n",
    "row_missing = (wr_loaded.isna() & off_mask).sum(axis=1).astype(int)\n",
    "row_ok = bool((row_missing <= allowed_per_row).all())\n",
    "print(f\"[POLICY] T0={T0} -> allowed_per_row={allowed_per_row} (MAX_NAN_RATIO={MAX_NAN_RATIO}, \"\n",
    "      f\"min={MIN_NAN_ALLOWED}, {'ceil' if USE_CEIL else 'floor'})\")\n",
    "\n",
    "print(f\"[BASIC] T_final={T} | off-diag tot={total_offdiag} | definite={defined_offdiag} | \"\n",
    "      f\"righe score={rows} | missing={missing_offdiag} ({missing_rate:.1%}) | \"\n",
    "      f\"rows_match_defined={rows_match_defined}\")\n",
    "\n",
    "# --- Entrambe le direzioni per le coppie presenti nello score ---\n",
    "pair_counts = score.apply(lambda r: tuple(sorted((r[\"Deck A\"], r[\"Deck B\"]))), axis=1).value_counts()\n",
    "both_dirs_ok = bool((pair_counts == 2).all())\n",
    "if not both_dirs_ok:\n",
    "    print(\"[DIRS] coppie nello score senza entrambe le direzioni (prime 10):\")\n",
    "    print(pair_counts[pair_counts != 2].head(10))\n",
    "\n",
    "# --- Coerenza riga-per-riga ---\n",
    "n_bad_rows = int((score[\"N\"] != (score[\"W\"] + score[\"L\"] + score[\"T\"]).astype(\"Int64\")).sum())\n",
    "den_row    = (score[\"W\"] + score[\"L\"]).astype(\"Int64\")\n",
    "wr_row_calc = np.where(den_row > 0, 100.0 * score[\"W\"].astype(float) / den_row.astype(float), np.nan)\n",
    "wr_row_bad  = int((np.abs(score[\"WR_dir\"].astype(float) - np.round(wr_row_calc, 2)) > EPS_WR).sum())\n",
    "winrate_bad = int((score.get(\"Winrate\", score[\"WR_dir\"]).round(2) != score[\"WR_dir\"].round(2)).sum())\n",
    "print(f\"[ROW] N‚â†W+L+T: {n_bad_rows} | |WR-Formula|>{EPS_WR:.2f}pp: {wr_row_bad} | Winrate‚â†WR_dir: {winrate_bad}\")\n",
    "\n",
    "# --- Matrici da score e confronti avanzati ---\n",
    "W = (score.pivot_table(index=\"Deck A\", columns=\"Deck B\", values=\"W\", aggfunc=\"sum\", fill_value=0)\n",
    "           .reindex(index=axis, columns=axis).astype(float))\n",
    "L = (score.pivot_table(index=\"Deck A\", columns=\"Deck B\", values=\"L\", aggfunc=\"sum\", fill_value=0)\n",
    "           .reindex(index=axis, columns=axis).astype(float))\n",
    "Tmat = (score.pivot_table(index=\"Deck A\", columns=\"Deck B\", values=\"T\", aggfunc=\"sum\", fill_value=0)\n",
    "            .reindex(index=axis, columns=axis).astype(float))\n",
    "\n",
    "den = W + L\n",
    "wr_calc = (W * 100.0 / den).where(den > 0).round(2)\n",
    "\n",
    "wr_loaded_cmp = wr_loaded.copy()\n",
    "np.fill_diagonal(wr_calc.values,       np.nan)\n",
    "np.fill_diagonal(wr_loaded_cmp.values, np.nan)\n",
    "\n",
    "both = wr_loaded_cmp.notna() & wr_calc.notna()\n",
    "n_cells = int(both.sum().sum())\n",
    "n_bad   = int(((wr_loaded_cmp.where(both) - wr_calc.where(both)).abs() > EPS_WR).sum().sum())\n",
    "print(f\"[WR=matrix] celle confrontate={n_cells} | fuori>{EPS_WR:.2f}pp: {n_bad}\")\n",
    "\n",
    "wr_sum = (wr_loaded + wr_loaded.T)\n",
    "np.fill_diagonal(wr_sum.values, np.nan)\n",
    "bad_100 = int(((wr_sum - 100.0).abs() > EPS_SUM).sum().sum())\n",
    "print(f\"[SUM=100] celle fuori>{EPS_SUM:.2f}pp: {bad_100}\")\n",
    "\n",
    "n_loaded = df_n.reindex(index=axis, columns=axis).astype(float)\n",
    "n_calc   = (W + L).astype(float)\n",
    "np.fill_diagonal(n_loaded.values, np.nan)\n",
    "np.fill_diagonal(n_calc.values,   np.nan)\n",
    "\n",
    "cmp_off  = (n_loaded.fillna(-1) != n_calc.fillna(-1)) & off_mask\n",
    "mm_off   = int(cmp_off.sum().sum())\n",
    "asym_off = int(((n_loaded - n_loaded.T).where(off_mask).fillna(0) != 0).sum().sum())\n",
    "W_vs_Lt_bad = int(((W - L.T).where(off_mask).fillna(0) != 0).sum().sum())\n",
    "T_sym_bad   = int(((Tmat - Tmat.T).where(off_mask).fillna(0) != 0).sum().sum())\n",
    "print(f\"[N_DIR] mismatch off: {mm_off} | asimmetrie off: {asym_off}\")\n",
    "print(f\"[SYM]   W‚â†L^T off: {W_vs_Lt_bad} | T‚â†T^T off: {T_sym_bad}\")\n",
    "\n",
    "# --- Esito (coerente con la policy del filtro calcolata su T0) ---\n",
    "ok = (\n",
    "    mirrors_ok and subset_ok and no_overfill and rows_match_defined and both_dirs_ok\n",
    "    and (missing_rate <= MAX_NAN_RATIO) and row_ok                # per-riga calcolato da T0\n",
    "    and n_bad_rows == 0 and wr_row_bad == 0 and winrate_bad == 0\n",
    "    and n_bad == 0 and bad_100 == 0 and mm_off == 0 and asym_off == 0\n",
    "    and W_vs_Lt_bad == 0 and T_sym_bad == 0\n",
    ")\n",
    "print(\"\\n=== VALIDATION:\", \"PASS ‚úÖ\" if ok else \"CHECK ‚ö†Ô∏è\", \"===\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
